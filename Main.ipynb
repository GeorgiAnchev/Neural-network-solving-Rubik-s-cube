{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Rubik2.Rubik2 import *\n",
    "from dataHelper import *\n",
    "from collections import deque\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb\n",
      "bb\n",
      "ww rr yy oo\n",
      "ww rr yy oo\n",
      "gg\n",
      "gg\n",
      "\n",
      "yy\n",
      "br\n",
      "rg yo bb rw\n",
      "bw rg yg oo\n",
      "wg\n",
      "wo\n",
      "\n",
      "yy\n",
      "br\n",
      "rg yo bb rw\n",
      "bw rg yg oo\n",
      "wg\n",
      "wo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cube = Rubik2()\n",
    "cube.show()\n",
    "cube.rotateBottom()\n",
    "cube.rotateLeft()\n",
    "cube.rotateBack()\n",
    "cube.show()\n",
    "asstring = cube.getStateCompact()\n",
    "newCube = Rubik2(asstring)\n",
    "newCube.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNeighbors(current_states, visited_states):\n",
    "    if(len(current_states) == 0): \n",
    "        return #empty queue\n",
    "    \n",
    "    (current_string, path_length) = current_states.popleft()\n",
    "    \n",
    "    for action in range(6):\n",
    "        state = Rubik2(current_string)\n",
    "        state.performMove(action)\n",
    "        new_state_string = state.getStateCompact()\n",
    "        \n",
    "        if(new_state_string in visited_states): #state is already visited\n",
    "            continue \n",
    "        current_states.append((new_state_string, path_length + 1))\n",
    "        visited_states[new_state_string] = (path_length + 1, action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited[solved_state.getStateCompact()] = (0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0\n",
      "10000 28280 38279\n",
      "20000 54398 74397\n",
      "30000 79329 109328\n",
      "40000 102945 142944\n",
      "50000 127112 177111\n",
      "60000 151546 211545\n",
      "70000 174678 244677\n",
      "80000 198024 278023\n",
      "90000 220195 310194\n",
      "100000 242301 342300\n",
      "110000 263381 373380\n",
      "120000 284268 404267\n",
      "130000 304292 434291\n",
      "140000 324137 464136\n",
      "150000 343813 493812\n",
      "160000 362601 522600\n",
      "170000 385888 555887\n",
      "180000 408206 588205\n",
      "190000 430185 620184\n",
      "200000 451042 651041\n",
      "210000 472058 682057\n",
      "220000 491806 711805\n",
      "230000 511727 741726\n",
      "240000 531024 771023\n",
      "250000 549766 799765\n",
      "260000 568932 828931\n",
      "270000 587631 857630\n",
      "280000 605815 885814\n",
      "290000 623165 913164\n",
      "300000 640292 940291\n",
      "310000 656799 966798\n",
      "320000 673813 993812\n",
      "330000 690309 1020308\n",
      "340000 706588 1046587\n",
      "350000 722031 1072030\n",
      "360000 736652 1096651\n",
      "370000 751521 1121520\n",
      "380000 765608 1145607\n",
      "390000 779859 1169858\n",
      "400000 793769 1193768\n",
      "410000 807193 1217192\n",
      "420000 819929 1239928\n",
      "430000 832842 1262841\n",
      "440000 845338 1285337\n",
      "450000 857502 1307501\n",
      "460000 869172 1329171\n",
      "470000 880580 1350579\n",
      "480000 890824 1370823\n",
      "490000 901604 1391603\n",
      "500000 911785 1411784\n",
      "510000 921640 1431639\n",
      "520000 931393 1451392\n",
      "530000 950803 1480802\n",
      "540000 969325 1509324\n",
      "550000 987887 1537886\n",
      "560000 1005440 1565439\n",
      "570000 1022311 1592310\n",
      "580000 1039290 1619289\n",
      "590000 1056124 1646123\n",
      "600000 1072547 1672546\n",
      "610000 1089044 1699043\n",
      "620000 1104791 1724790\n",
      "630000 1119384 1749383\n",
      "640000 1134137 1774136\n",
      "650000 1148439 1798438\n",
      "660000 1162884 1822883\n",
      "670000 1176720 1846719\n",
      "680000 1190165 1870164\n",
      "690000 1203047 1893046\n",
      "700000 1215726 1915725\n",
      "710000 1228415 1938414\n",
      "720000 1240854 1960853\n",
      "730000 1252719 1982718\n",
      "740000 1264144 2004143\n",
      "750000 1274513 2024512\n",
      "760000 1285328 2045327\n",
      "770000 1295518 2065517\n",
      "780000 1305595 2085594\n",
      "790000 1314904 2104903\n",
      "800000 1324420 2124419\n",
      "810000 1333672 2143671\n",
      "820000 1342449 2162448\n",
      "830000 1350662 2180661\n",
      "840000 1358473 2198472\n",
      "850000 1365818 2215817\n",
      "860000 1373267 2233266\n",
      "870000 1380390 2250389\n",
      "880000 1386984 2266983\n",
      "890000 1393008 2283007\n",
      "900000 1399568 2299567\n",
      "910000 1405645 2315644\n",
      "920000 1411220 2331219\n",
      "930000 1416338 2346337\n",
      "940000 1421252 2361251\n",
      "950000 1425724 2375723\n",
      "960000 1430004 2390003\n",
      "970000 1433875 2403874\n",
      "980000 1438342 2418341\n",
      "990000 1442078 2432077\n",
      "1000000 1445400 2445399\n",
      "1010000 1448429 2458428\n",
      "1020000 1451350 2471349\n",
      "1030000 1453968 2483967\n",
      "1040000 1456067 2496066\n",
      "1050000 1458059 2508058\n",
      "1060000 1460010 2520009\n",
      "1070000 1461397 2531396\n",
      "1080000 1462638 2542637\n",
      "1090000 1463685 2553684\n",
      "1100000 1464453 2564452\n",
      "1110000 1464550 2574549\n",
      "1120000 1464791 2584790\n",
      "1130000 1464639 2594638\n",
      "1140000 1464197 2604196\n",
      "1150000 1463996 2613995\n",
      "1160000 1463310 2623309\n",
      "1170000 1462342 2632341\n",
      "1180000 1460978 2640977\n",
      "1190000 1459591 2649590\n",
      "1200000 1457904 2657903\n",
      "1210000 1456189 2666188\n",
      "1220000 1453960 2673959\n",
      "1230000 1451569 2681568\n",
      "1240000 1448898 2688897\n",
      "1250000 1445992 2695991\n",
      "1260000 1443231 2703230\n",
      "1270000 1440047 2710046\n",
      "1280000 1436555 2716554\n",
      "1290000 1432988 2722987\n",
      "1300000 1429308 2729307\n",
      "1310000 1425366 2735365\n",
      "1320000 1421226 2741225\n",
      "1330000 1416865 2746864\n",
      "1340000 1412235 2752234\n",
      "1350000 1407485 2757484\n",
      "1360000 1402633 2762632\n",
      "1370000 1397499 2767498\n",
      "1380000 1392436 2772435\n",
      "1390000 1387167 2777166\n",
      "1400000 1381669 2781668\n",
      "1410000 1375801 2785800\n",
      "1420000 1369961 2789960\n",
      "1430000 1363915 2793914\n",
      "1440000 1357667 2797666\n",
      "1450000 1351002 2801001\n",
      "1460000 1360161 2820160\n",
      "1470000 1369472 2839471\n",
      "1480000 1378200 2858199\n",
      "1490000 1386286 2876285\n",
      "1500000 1393988 2893987\n",
      "1510000 1401323 2911322\n",
      "1520000 1408647 2928646\n",
      "1530000 1415751 2945750\n",
      "1540000 1422187 2962186\n",
      "1550000 1428519 2978518\n",
      "1560000 1434865 2994864\n",
      "1570000 1440722 3010721\n",
      "1580000 1446347 3026346\n",
      "1590000 1451342 3041341\n",
      "1600000 1456239 3056238\n",
      "1610000 1460750 3070749\n",
      "1620000 1464831 3084830\n",
      "1630000 1468824 3098823\n",
      "1640000 1473175 3113174\n",
      "1650000 1476842 3126841\n",
      "1660000 1480170 3140169\n",
      "1670000 1483163 3153162\n",
      "1680000 1485927 3165926\n",
      "1690000 1488597 3178596\n",
      "1700000 1490627 3190626\n",
      "1710000 1492654 3202653\n",
      "1720000 1494454 3214453\n",
      "1730000 1495800 3225799\n",
      "1740000 1496978 3236977\n",
      "1750000 1498025 3248024\n",
      "1760000 1498696 3258695\n",
      "1770000 1498736 3268735\n",
      "1780000 1498995 3278994\n",
      "1790000 1498755 3288754\n",
      "1800000 1498413 3298412\n",
      "1810000 1498098 3308097\n",
      "1820000 1497385 3317384\n",
      "1830000 1496321 3326320\n",
      "1840000 1494980 3334979\n",
      "1850000 1493562 3343561\n",
      "1860000 1491792 3351791\n",
      "1870000 1490085 3360084\n",
      "1880000 1487863 3367862\n",
      "1890000 1485340 3375339\n",
      "1900000 1482632 3382631\n",
      "1910000 1479640 3389639\n",
      "1920000 1476889 3396888\n",
      "1930000 1473675 3403674\n",
      "1940000 1470185 3410184\n",
      "1950000 1466620 3416619\n",
      "1960000 1462859 3422858\n",
      "1970000 1458880 3428879\n",
      "1980000 1454771 3434770\n",
      "1990000 1450367 3440366\n",
      "2000000 1445689 3445688\n",
      "2010000 1440877 3450876\n",
      "2020000 1435995 3455994\n",
      "2030000 1430818 3460817\n",
      "2040000 1425765 3465764\n",
      "2050000 1420442 3470441\n",
      "2060000 1414878 3474877\n",
      "2070000 1409003 3479002\n",
      "2080000 1403154 3483153\n",
      "2090000 1397069 3487068\n",
      "2100000 1390757 3490756\n",
      "2110000 1384210 3494209\n",
      "2120000 1378451 3498450\n",
      "2130000 1372466 3502465\n",
      "2140000 1366282 3506281\n",
      "2150000 1360089 3510088\n",
      "2160000 1353594 3513593\n",
      "2170000 1346800 3516799\n",
      "2180000 1340090 3520089\n",
      "2190000 1333317 3523316\n",
      "2200000 1326357 3526356\n",
      "2210000 1319314 3529313\n",
      "2220000 1312176 3532175\n",
      "2230000 1304948 3534947\n",
      "2240000 1297545 3537544\n",
      "2250000 1290054 3540053\n",
      "2260000 1282453 3542452\n",
      "2270000 1274705 3544704\n",
      "2280000 1266831 3546830\n",
      "2290000 1258894 3548893\n",
      "2300000 1250838 3550837\n",
      "2310000 1242779 3552778\n",
      "2320000 1234581 3554580\n",
      "2330000 1226243 3556242\n",
      "2340000 1217845 3557844\n",
      "2350000 1209160 3559159\n",
      "2360000 1200563 3560562\n",
      "2370000 1191925 3561924\n",
      "2380000 1183183 3563182\n",
      "2390000 1174281 3564280\n",
      "2400000 1165328 3565327\n",
      "2410000 1156628 3566627\n",
      "2420000 1147886 3567885\n",
      "2430000 1139044 3569043\n",
      "2440000 1130107 3570106\n",
      "2450000 1121122 3571121\n",
      "2460000 1112054 3572053\n",
      "2470000 1102917 3572916\n",
      "2480000 1093747 3573746\n",
      "2490000 1084604 3574603\n",
      "2500000 1075284 3575283\n",
      "2510000 1065965 3575964\n",
      "2520000 1056638 3576637\n",
      "2530000 1047268 3577267\n",
      "2540000 1037844 3577843\n",
      "2550000 1028361 3578360\n",
      "2560000 1018833 3578832\n",
      "2570000 1009279 3579278\n",
      "2580000 999726 3579725\n",
      "2590000 990079 3580078\n",
      "2600000 980415 3580414\n",
      "2610000 970713 3580712\n",
      "2620000 961026 3581025\n",
      "2630000 951350 3581349\n",
      "2640000 941615 3581614\n",
      "2650000 931870 3581869\n",
      "2660000 922078 3582077\n",
      "2670000 912277 3582276\n",
      "2680000 902451 3582450\n",
      "2690000 892595 3582594\n",
      "2700000 882731 3582730\n",
      "2710000 872911 3582910\n",
      "2720000 863067 3583066\n",
      "2730000 853175 3583174\n",
      "2740000 843277 3583276\n",
      "2750000 833358 3583357\n",
      "2760000 823428 3583427\n",
      "2770000 813480 3583479\n",
      "2780000 803527 3583526\n",
      "2790000 793581 3583580\n",
      "2800000 783604 3583603\n",
      "2810000 777398 3587397\n",
      "2820000 771410 3591409\n",
      "2830000 765293 3595292\n",
      "2840000 759214 3599213\n",
      "2850000 752762 3602761\n",
      "2860000 746013 3606012\n",
      "2870000 739236 3609235\n",
      "2880000 732565 3612564\n",
      "2890000 725566 3615565\n",
      "2900000 718554 3618553\n",
      "2910000 711407 3621406\n",
      "2920000 704234 3624233\n",
      "2930000 696847 3626846\n",
      "2940000 689401 3629400\n",
      "2950000 681887 3631886\n",
      "2960000 674147 3634146\n",
      "2970000 666289 3636288\n",
      "2980000 658357 3638356\n",
      "2990000 650373 3640372\n",
      "3000000 642299 3642298\n",
      "3010000 634146 3644145\n",
      "3020000 625851 3645850\n",
      "3030000 617427 3647426\n",
      "3040000 608847 3648846\n",
      "3050000 600183 3650182\n",
      "3060000 591596 3651595\n",
      "3070000 582831 3652830\n",
      "3080000 574040 3654039\n",
      "3090000 565091 3655090\n",
      "3100000 556297 3656296\n",
      "3110000 547571 3657570\n",
      "3120000 538763 3658762\n",
      "3130000 529843 3659842\n",
      "3140000 520871 3660870\n",
      "3150000 511813 3661812\n",
      "3160000 502689 3662688\n",
      "3170000 493550 3663549\n",
      "3180000 484388 3664387\n",
      "3190000 475114 3665113\n",
      "3200000 465791 3665790\n",
      "3210000 456471 3666470\n",
      "3220000 447120 3667119\n",
      "3230000 437710 3667709\n",
      "3240000 428233 3668232\n",
      "3250000 418720 3668719\n",
      "3260000 409198 3669197\n",
      "3270000 399610 3669609\n",
      "3280000 390000 3669999\n",
      "3290000 380342 3670341\n",
      "3300000 370649 3670648\n",
      "3310000 360950 3670949\n",
      "3320000 351276 3671275\n",
      "3330000 341541 3671540\n",
      "3340000 331800 3671799\n",
      "3350000 322026 3672025\n",
      "3360000 312238 3672237\n",
      "3370000 302413 3672412\n",
      "3380000 292563 3672562\n",
      "3390000 282698 3672697\n",
      "3400000 272863 3672862\n",
      "3410000 263033 3673032\n",
      "3420000 253155 3673154\n",
      "3430000 243261 3673260\n",
      "3440000 233349 3673348\n",
      "3450000 223413 3673412\n",
      "3460000 213473 3673472\n",
      "3470000 203519 3673518\n",
      "3480000 193571 3673570\n",
      "3490000 183605 3673604\n",
      "3500000 173662 3673661\n",
      "3510000 163749 3673748\n",
      "3520000 153787 3673786\n",
      "3530000 143823 3673822\n",
      "3540000 133845 3673844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3550000 123861 3673860\n",
      "3560000 113877 3673876\n",
      "3570000 103883 3673882\n",
      "3580000 93885 3673884\n",
      "3590000 83938 3673937\n",
      "3600000 74025 3674024\n",
      "3610000 64063 3674062\n",
      "3620000 54099 3674098\n",
      "3630000 44121 3674120\n",
      "3640000 34137 3674136\n",
      "3650000 24153 3674152\n",
      "3660000 14159 3674158\n",
      "3670000 4161 3674160\n"
     ]
    }
   ],
   "source": [
    "visited = dict()\n",
    "solved_state = Rubik2().getStateCompact()\n",
    "current_states = deque()\n",
    "current_states.append((solved_state, 0))\n",
    "visited[solved_state] = (0, -1)\n",
    "\n",
    "i = 0\n",
    "while current_states:\n",
    "    if(i % 10000 == 0) :\n",
    "        print(i, len(current_states), len(visited))\n",
    "        \n",
    "    addNeighbors(current_states, visited)\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data1.csv\", \"w\", newline='') as fp:\n",
    "    w = csv.writer(fp)\n",
    "    for key, (steps, move) in visited.items():\n",
    "        w.writerow([key, move, steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data1.csv\") as fp:\n",
    "    reader = csv.reader(fp, delimiter=\",\", quotechar='\"')\n",
    "    data2 = dict((rows[0], (int(rows[1]), int(rows[2]))) for rows in reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3674160"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({11: 1350852, 10: 930588, 12: 782536, 9: 360508, 8: 114149, 13: 90280, 7: 33058, 6: 8969, 5: 2256, 4: 534, 14: 276, 3: 120, 2: 27, 1: 6, 0: 1})\n"
     ]
    }
   ],
   "source": [
    "showDistribution(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedData = balanceData(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9818512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balancedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({11: 1350852, 10: 930588, 12: 782536, 1: 675426, 2: 675426, 3: 675426, 4: 675426, 5: 675426, 6: 675426, 7: 675426, 8: 675426, 9: 675426, 13: 675426, 14: 276})\n"
     ]
    }
   ],
   "source": [
    "showDistributionList(balancedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(balancedData[0: 3])\n",
    "random.shuffle(balancedData)\n",
    "#print(balancedData[0: 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "moveLabels, cubesAsVectors, _ = transformDataIntoExamples(balancedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_moves = 6   # const\n",
    "\n",
    "data_len = len(balancedData)\n",
    "train_len = data_len\n",
    "val_len = int(data_len * 0.01)\n",
    "\n",
    "train_data = cubesAsVectors\n",
    "train_labels = moveLabels\n",
    "train_labels_one_hot = (np.arange(num_moves) == train_labels.reshape(-1, 1)).astype(np.float32)\n",
    "\n",
    "val_data = cubesAsVectors[:val_len]\n",
    "val_labels = moveLabels[:val_len]\n",
    "val_labels_one_hot = (np.arange(num_moves) == val_labels.reshape(-1, 1)).astype(np.float32)\n",
    "\n",
    "train_indices = range(train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch(batch_size):\n",
    "    mask = np.asarray(random.sample(train_indices, batch_size))\n",
    "    \n",
    "    batch = train_data[mask]\n",
    "    labels = train_labels_one_hot[mask]\n",
    "    \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "hidden1 = 2048\n",
    "hidden2 = 1024\n",
    "hidden3 = 512\n",
    "\n",
    "num_colors = 6   # const\n",
    "num_sides = 6   # const\n",
    "num_stickers = 21 # const # 3 sides with 3 stickers and 3 sides with 4 stickers\n",
    "num_input_dimensions = num_stickers * num_colors # 126 - const\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_input_dimensions))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_moves))\n",
    "    tf_valid_dataset = tf.constant(val_data)\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([num_input_dimensions, hidden1], stddev=0.1), name=\"W1\")\n",
    "    b1 = tf.Variable(tf.zeros([hidden1]), name=\"b1\")\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden1, hidden2], stddev=0.1), name=\"W2\")\n",
    "    b2 = tf.Variable(tf.zeros([hidden2]), name=\"b2\")\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([hidden2, hidden3], stddev=0.1), name=\"W3\")\n",
    "    b3 = tf.Variable(tf.zeros([hidden3]), name=\"b3\")\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([hidden3, num_moves], stddev=0.1), name=\"W4\")\n",
    "    b4 = tf.Variable(tf.zeros([num_moves]), name=\"b4\")\n",
    "    \n",
    "    def model(data):\n",
    "        layer1_raw = tf.matmul(data, W1) + b1\n",
    "        layer1 = tf.nn.relu(layer1_raw)\n",
    "        \n",
    "        layer2_raw = tf.matmul(layer1, W2) + b2\n",
    "        layer2 = tf.nn.relu(layer2_raw)\n",
    "        \n",
    "        layer3_raw = tf.matmul(layer2, W3) + b3\n",
    "        layer3 = tf.nn.relu(layer3_raw)\n",
    "        \n",
    "        logits = tf.matmul(layer3, W4) + b4\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(labels, logits):\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    loss = compute_loss(tf_train_labels, logits)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training and validation data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 18.1%\n",
      "Initialized\n",
      "Average loss at step 2000: 1.302542\n",
      "Average loss at step 4000: 1.173354\n",
      "Average loss at step 6000: 1.138110\n",
      "Average loss at step 8000: 1.110272\n",
      "Average loss at step 10000: 1.090744\n",
      "Average loss at step 12000: 1.075896\n",
      "Average loss at step 14000: 1.065093\n",
      "Average loss at step 16000: 1.050598\n",
      "Average loss at step 18000: 1.045570\n",
      "Average loss at step 20000: 1.034380\n",
      "Average loss at step 22000: 1.026370\n",
      "Average loss at step 24000: 1.019291\n",
      "Average loss at step 26000: 1.014642\n",
      "Average loss at step 28000: 1.006135\n",
      "Average loss at step 30000: 1.004191\n",
      "Average loss at step 32000: 0.999642\n",
      "Average loss at step 34000: 0.992518\n",
      "Average loss at step 36000: 0.987855\n",
      "Average loss at step 38000: 0.982152\n",
      "Average loss at step 40000: 0.980181\n",
      "Average loss at step 42000: 0.978646\n",
      "Average loss at step 44000: 0.972558\n",
      "Average loss at step 46000: 0.970183\n",
      "Average loss at step 48000: 0.964541\n",
      "Average loss at step 50000: 0.965616\n",
      "Average loss at step 52000: 0.958193\n",
      "Average loss at step 54000: 0.957625\n",
      "Average loss at step 56000: 0.955114\n",
      "Average loss at step 58000: 0.949937\n",
      "Average loss at step 60000: 0.946416\n",
      "Average loss at step 62000: 0.944709\n",
      "Average loss at step 64000: 0.941898\n",
      "Average loss at step 66000: 0.939879\n",
      "Average loss at step 68000: 0.936416\n",
      "Average loss at step 70000: 0.933575\n",
      "Average loss at step 72000: 0.926874\n",
      "Average loss at step 74000: 0.934021\n",
      "Average loss at step 76000: 0.929289\n",
      "Average loss at step 78000: 0.924022\n",
      "Average loss at step 80000: 0.925948\n",
      "Average loss at step 82000: 0.925153\n",
      "Average loss at step 84000: 0.917726\n",
      "Average loss at step 86000: 0.920681\n",
      "Average loss at step 88000: 0.920963\n",
      "Average loss at step 90000: 0.914861\n",
      "Average loss at step 92000: 0.910369\n",
      "Average loss at step 94000: 0.911111\n",
      "Average loss at step 96000: 0.908261\n",
      "Average loss at step 98000: 0.906090\n",
      "Average loss at step 100000: 0.904988\n",
      "Average loss at step 102000: 0.906573\n",
      "Average loss at step 104000: 0.900817\n",
      "Average loss at step 106000: 0.899958\n",
      "Average loss at step 108000: 0.902549\n",
      "Average loss at step 110000: 0.897820\n",
      "Average loss at step 112000: 0.897585\n",
      "Average loss at step 114000: 0.895237\n",
      "Average loss at step 116000: 0.895310\n",
      "Average loss at step 118000: 0.889601\n",
      "Average loss at step 120000: 0.892343\n",
      "Average loss at step 122000: 0.891881\n",
      "Average loss at step 124000: 0.888655\n",
      "Average loss at step 126000: 0.889931\n",
      "Average loss at step 128000: 0.885246\n",
      "Average loss at step 130000: 0.885415\n",
      "Average loss at step 132000: 0.880880\n",
      "Average loss at step 134000: 0.881752\n",
      "Average loss at step 136000: 0.880183\n",
      "Average loss at step 138000: 0.878701\n",
      "Average loss at step 140000: 0.878992\n",
      "Average loss at step 142000: 0.873209\n",
      "Average loss at step 144000: 0.871934\n",
      "Average loss at step 146000: 0.873377\n",
      "Average loss at step 148000: 0.873648\n",
      "Average loss at step 150000: 0.871005\n",
      "Average loss at step 152000: 0.869693\n",
      "Average loss at step 154000: 0.869378\n",
      "Average loss at step 156000: 0.867211\n",
      "Average loss at step 158000: 0.867893\n",
      "Average loss at step 160000: 0.865033\n",
      "Average loss at step 162000: 0.862072\n",
      "Average loss at step 164000: 0.865028\n",
      "Average loss at step 166000: 0.863719\n",
      "Average loss at step 168000: 0.859412\n",
      "Average loss at step 170000: 0.860255\n",
      "Average loss at step 172000: 0.858795\n",
      "Average loss at step 174000: 0.858806\n",
      "Average loss at step 176000: 0.858159\n",
      "Average loss at step 178000: 0.856260\n",
      "Average loss at step 180000: 0.854036\n",
      "Average loss at step 182000: 0.851776\n",
      "Average loss at step 184000: 0.854001\n",
      "Average loss at step 186000: 0.849217\n",
      "Average loss at step 188000: 0.845630\n",
      "Average loss at step 190000: 0.850989\n",
      "Average loss at step 192000: 0.850259\n",
      "Average loss at step 194000: 0.845980\n",
      "Average loss at step 196000: 0.847159\n",
      "Average loss at step 198000: 0.845982\n",
      "Average loss at step 200000: 0.847234\n",
      "Average loss at step 202000: 0.844758\n",
      "Average loss at step 204000: 0.840319\n",
      "Average loss at step 206000: 0.843095\n",
      "Average loss at step 208000: 0.841701\n",
      "Average loss at step 210000: 0.842940\n",
      "Average loss at step 212000: 0.837261\n",
      "Average loss at step 214000: 0.841203\n",
      "Average loss at step 216000: 0.835213\n",
      "Average loss at step 218000: 0.838052\n",
      "Average loss at step 220000: 0.832325\n",
      "Average loss at step 222000: 0.834592\n",
      "Average loss at step 224000: 0.831294\n",
      "Average loss at step 226000: 0.829129\n",
      "Average loss at step 228000: 0.830501\n",
      "Average loss at step 230000: 0.830048\n",
      "Average loss at step 232000: 0.834758\n",
      "Average loss at step 234000: 0.830023\n",
      "Average loss at step 236000: 0.833021\n",
      "Average loss at step 238000: 0.826979\n",
      "Average loss at step 240000: 0.826775\n",
      "Average loss at step 242000: 0.828345\n",
      "Average loss at step 244000: 0.828131\n",
      "Average loss at step 246000: 0.822012\n",
      "Average loss at step 248000: 0.822028\n",
      "Average loss at step 250000: 0.823507\n",
      "Average loss at step 252000: 0.822167\n",
      "Average loss at step 254000: 0.820944\n",
      "Average loss at step 256000: 0.823080\n",
      "Average loss at step 258000: 0.823438\n",
      "Average loss at step 260000: 0.817352\n",
      "Average loss at step 262000: 0.816821\n",
      "Average loss at step 264000: 0.816402\n",
      "Average loss at step 266000: 0.817667\n",
      "Average loss at step 268000: 0.815128\n",
      "Average loss at step 270000: 0.812837\n",
      "Average loss at step 272000: 0.814049\n",
      "Average loss at step 274000: 0.814705\n",
      "Average loss at step 276000: 0.811682\n",
      "Average loss at step 278000: 0.806694\n",
      "Average loss at step 280000: 0.812382\n",
      "Average loss at step 282000: 0.809501\n",
      "Average loss at step 284000: 0.805674\n",
      "Average loss at step 286000: 0.806693\n",
      "Average loss at step 288000: 0.805755\n",
      "Average loss at step 290000: 0.811152\n",
      "Average loss at step 292000: 0.804961\n",
      "Average loss at step 294000: 0.807567\n",
      "Average loss at step 296000: 0.804139\n",
      "Average loss at step 298000: 0.804309\n",
      "Average loss at step 300000: 0.802344\n",
      "Average loss at step 302000: 0.803103\n",
      "Average loss at step 304000: 0.798730\n",
      "Average loss at step 306000: 0.803529\n",
      "Average loss at step 308000: 0.801250\n",
      "Average loss at step 310000: 0.801203\n",
      "Average loss at step 312000: 0.800104\n",
      "Average loss at step 314000: 0.798127\n",
      "Average loss at step 316000: 0.796870\n",
      "Average loss at step 318000: 0.796989\n",
      "Average loss at step 320000: 0.796053\n",
      "Average loss at step 322000: 0.795666\n",
      "Average loss at step 324000: 0.793749\n",
      "Average loss at step 326000: 0.791291\n",
      "Average loss at step 328000: 0.790814\n",
      "Average loss at step 330000: 0.790951\n",
      "Average loss at step 332000: 0.792255\n",
      "Average loss at step 334000: 0.792025\n",
      "Average loss at step 336000: 0.796023\n",
      "Average loss at step 338000: 0.792086\n",
      "Average loss at step 340000: 0.786688\n",
      "Average loss at step 342000: 0.790487\n",
      "Average loss at step 344000: 0.786715\n",
      "Average loss at step 346000: 0.785304\n",
      "Average loss at step 348000: 0.783995\n",
      "Average loss at step 350000: 0.784949\n",
      "Average loss at step 352000: 0.789257\n",
      "Average loss at step 354000: 0.784987\n",
      "Average loss at step 356000: 0.783552\n",
      "Average loss at step 358000: 0.784861\n",
      "Average loss at step 360000: 0.782926\n",
      "Average loss at step 362000: 0.781905\n",
      "Average loss at step 364000: 0.781351\n",
      "Average loss at step 366000: 0.779831\n",
      "Average loss at step 368000: 0.777266\n",
      "Average loss at step 370000: 0.782254\n",
      "Average loss at step 372000: 0.778324\n",
      "Average loss at step 374000: 0.778460\n",
      "Average loss at step 376000: 0.778737\n",
      "Average loss at step 378000: 0.778126\n",
      "Average loss at step 380000: 0.776369\n",
      "Average loss at step 382000: 0.779253\n",
      "Average loss at step 384000: 0.774051\n",
      "Average loss at step 386000: 0.775890\n",
      "Average loss at step 388000: 0.770894\n",
      "Average loss at step 390000: 0.772149\n",
      "Average loss at step 392000: 0.773448\n",
      "Average loss at step 394000: 0.775381\n",
      "Average loss at step 396000: 0.770683\n",
      "Average loss at step 398000: 0.774023\n",
      "Average loss at step 400000: 0.771798\n",
      "Average loss at step 402000: 0.770149\n",
      "Average loss at step 404000: 0.768819\n",
      "Average loss at step 406000: 0.768286\n",
      "Average loss at step 408000: 0.766361\n",
      "Average loss at step 410000: 0.769106\n",
      "Average loss at step 412000: 0.767095\n",
      "Average loss at step 414000: 0.765304\n",
      "Average loss at step 416000: 0.764822\n",
      "Average loss at step 418000: 0.765501\n",
      "Average loss at step 420000: 0.761815\n",
      "Average loss at step 422000: 0.764362\n",
      "Average loss at step 424000: 0.763181\n",
      "Average loss at step 426000: 0.761624\n",
      "Average loss at step 428000: 0.763148\n",
      "Average loss at step 430000: 0.762656\n",
      "Average loss at step 432000: 0.758971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 434000: 0.761553\n",
      "Average loss at step 436000: 0.757594\n",
      "Average loss at step 438000: 0.762740\n",
      "Average loss at step 440000: 0.759268\n",
      "Average loss at step 442000: 0.760181\n",
      "Average loss at step 444000: 0.753873\n",
      "Average loss at step 446000: 0.757221\n",
      "Average loss at step 448000: 0.758498\n",
      "Average loss at step 450000: 0.759284\n",
      "Average loss at step 452000: 0.753731\n",
      "Average loss at step 454000: 0.754568\n",
      "Average loss at step 456000: 0.756347\n",
      "Average loss at step 458000: 0.754664\n",
      "Average loss at step 460000: 0.752232\n",
      "Average loss at step 462000: 0.757098\n",
      "Average loss at step 464000: 0.753096\n",
      "Average loss at step 466000: 0.753564\n",
      "Average loss at step 468000: 0.752342\n",
      "Average loss at step 470000: 0.751159\n",
      "Average loss at step 472000: 0.746238\n",
      "Average loss at step 474000: 0.749721\n",
      "Average loss at step 476000: 0.747664\n",
      "Average loss at step 478000: 0.749576\n",
      "Average loss at step 480000: 0.747718\n",
      "Average loss at step 482000: 0.747442\n",
      "Average loss at step 484000: 0.747950\n",
      "Average loss at step 486000: 0.746406\n",
      "Average loss at step 488000: 0.743389\n",
      "Average loss at step 490000: 0.746560\n",
      "Average loss at step 492000: 0.744760\n",
      "Average loss at step 494000: 0.742157\n",
      "Average loss at step 496000: 0.744105\n",
      "Average loss at step 498000: 0.744870\n",
      "Average loss at step 500000: 0.741435\n",
      "Average loss at step 502000: 0.742343\n",
      "Average loss at step 504000: 0.743176\n",
      "Average loss at step 506000: 0.741438\n",
      "Average loss at step 508000: 0.737966\n",
      "Average loss at step 510000: 0.739711\n",
      "Average loss at step 512000: 0.739231\n",
      "Average loss at step 514000: 0.740100\n",
      "Average loss at step 516000: 0.737677\n",
      "Average loss at step 518000: 0.740124\n",
      "Average loss at step 520000: 0.738664\n",
      "Average loss at step 522000: 0.737844\n",
      "Average loss at step 524000: 0.734983\n",
      "Average loss at step 526000: 0.734373\n",
      "Average loss at step 528000: 0.737195\n",
      "Average loss at step 530000: 0.734816\n",
      "Average loss at step 532000: 0.733915\n",
      "Average loss at step 534000: 0.733536\n",
      "Average loss at step 536000: 0.735400\n",
      "Average loss at step 538000: 0.734725\n",
      "Average loss at step 540000: 0.733310\n",
      "Average loss at step 542000: 0.732311\n",
      "Average loss at step 544000: 0.730960\n",
      "Average loss at step 546000: 0.732556\n",
      "Average loss at step 548000: 0.731529\n",
      "Average loss at step 550000: 0.729849\n",
      "Average loss at step 552000: 0.731897\n",
      "Average loss at step 554000: 0.729140\n",
      "Average loss at step 556000: 0.729937\n",
      "Average loss at step 558000: 0.727066\n",
      "Average loss at step 560000: 0.726015\n",
      "Average loss at step 562000: 0.725965\n",
      "Average loss at step 564000: 0.727539\n",
      "Average loss at step 566000: 0.727239\n",
      "Average loss at step 568000: 0.725615\n",
      "Average loss at step 570000: 0.724710\n",
      "Average loss at step 572000: 0.723839\n",
      "Average loss at step 574000: 0.724893\n",
      "Average loss at step 576000: 0.725876\n",
      "Average loss at step 578000: 0.723583\n",
      "Average loss at step 580000: 0.723371\n",
      "Average loss at step 582000: 0.724094\n",
      "Average loss at step 584000: 0.722952\n",
      "Average loss at step 586000: 0.722879\n",
      "Average loss at step 588000: 0.720604\n",
      "Average loss at step 590000: 0.719430\n",
      "Average loss at step 592000: 0.720903\n",
      "Average loss at step 594000: 0.720243\n",
      "Average loss at step 596000: 0.719985\n",
      "Average loss at step 598000: 0.721650\n",
      "Average loss at step 600000: 0.719893\n",
      "Average loss at step 602000: 0.720107\n",
      "Average loss at step 604000: 0.719340\n",
      "Average loss at step 606000: 0.719331\n",
      "Average loss at step 608000: 0.718320\n",
      "Average loss at step 610000: 0.718784\n",
      "Average loss at step 612000: 0.714554\n",
      "Average loss at step 614000: 0.716808\n",
      "Average loss at step 616000: 0.713859\n",
      "Average loss at step 618000: 0.716054\n",
      "Average loss at step 620000: 0.714121\n",
      "Average loss at step 622000: 0.711565\n",
      "Average loss at step 624000: 0.714606\n",
      "Average loss at step 626000: 0.714497\n",
      "Average loss at step 628000: 0.715278\n",
      "Average loss at step 630000: 0.712867\n",
      "Average loss at step 632000: 0.710998\n",
      "Average loss at step 634000: 0.709147\n",
      "Average loss at step 636000: 0.712935\n",
      "Average loss at step 638000: 0.708195\n",
      "Average loss at step 640000: 0.711268\n",
      "Average loss at step 642000: 0.710340\n",
      "Average loss at step 644000: 0.709103\n",
      "Average loss at step 646000: 0.708678\n",
      "Average loss at step 648000: 0.706011\n",
      "Average loss at step 650000: 0.710847\n",
      "Average loss at step 652000: 0.706407\n",
      "Average loss at step 654000: 0.707050\n",
      "Average loss at step 656000: 0.703928\n",
      "Average loss at step 658000: 0.709762\n",
      "Average loss at step 660000: 0.706886\n",
      "Average loss at step 662000: 0.706225\n",
      "Average loss at step 664000: 0.701140\n",
      "Average loss at step 666000: 0.705478\n",
      "Average loss at step 668000: 0.704163\n",
      "Average loss at step 670000: 0.705837\n",
      "Average loss at step 672000: 0.701414\n",
      "Average loss at step 674000: 0.701115\n",
      "Average loss at step 676000: 0.706022\n",
      "Average loss at step 678000: 0.704187\n",
      "Average loss at step 680000: 0.701673\n",
      "Average loss at step 682000: 0.700243\n",
      "Average loss at step 684000: 0.699049\n",
      "Average loss at step 686000: 0.700301\n",
      "Average loss at step 688000: 0.700827\n",
      "Average loss at step 690000: 0.700172\n",
      "Average loss at step 692000: 0.703084\n",
      "Average loss at step 694000: 0.698678\n",
      "Average loss at step 696000: 0.699778\n",
      "Average loss at step 698000: 0.700039\n",
      "Average loss at step 700000: 0.701625\n",
      "Average loss at step 702000: 0.698011\n",
      "Average loss at step 704000: 0.694122\n",
      "Average loss at step 706000: 0.699470\n",
      "Average loss at step 708000: 0.692841\n",
      "Average loss at step 710000: 0.697315\n",
      "Average loss at step 712000: 0.695301\n",
      "Average loss at step 714000: 0.696043\n",
      "Average loss at step 716000: 0.696812\n",
      "Average loss at step 718000: 0.694594\n",
      "Average loss at step 720000: 0.697636\n",
      "Average loss at step 722000: 0.692622\n",
      "Average loss at step 724000: 0.692302\n",
      "Average loss at step 726000: 0.690657\n",
      "Average loss at step 728000: 0.690050\n",
      "Average loss at step 730000: 0.693932\n",
      "Average loss at step 732000: 0.693807\n",
      "Average loss at step 734000: 0.693026\n",
      "Average loss at step 736000: 0.692211\n",
      "Average loss at step 738000: 0.693567\n",
      "Average loss at step 740000: 0.689257\n",
      "Average loss at step 742000: 0.689789\n",
      "Average loss at step 744000: 0.686104\n",
      "Average loss at step 746000: 0.688568\n",
      "Average loss at step 748000: 0.687968\n",
      "Average loss at step 750000: 0.690033\n",
      "Average loss at step 752000: 0.686521\n",
      "Average loss at step 754000: 0.687787\n",
      "Average loss at step 756000: 0.686601\n",
      "Average loss at step 758000: 0.684053\n",
      "Average loss at step 760000: 0.686237\n",
      "Average loss at step 762000: 0.686365\n",
      "Average loss at step 764000: 0.685768\n",
      "Average loss at step 766000: 0.689243\n",
      "Average loss at step 768000: 0.684574\n",
      "Average loss at step 770000: 0.683737\n",
      "Average loss at step 772000: 0.685191\n",
      "Average loss at step 774000: 0.685472\n",
      "Average loss at step 776000: 0.683768\n",
      "Average loss at step 778000: 0.685034\n",
      "Average loss at step 780000: 0.684335\n",
      "Average loss at step 782000: 0.679009\n",
      "Average loss at step 784000: 0.679892\n",
      "Average loss at step 786000: 0.684693\n",
      "Average loss at step 788000: 0.677531\n",
      "Average loss at step 790000: 0.681200\n",
      "Average loss at step 792000: 0.681054\n",
      "Average loss at step 794000: 0.683671\n",
      "Average loss at step 796000: 0.681619\n",
      "Average loss at step 798000: 0.680049\n",
      "Average loss at step 800000: 0.679396\n",
      "Average loss at step 802000: 0.679726\n",
      "Average loss at step 804000: 0.677555\n",
      "Average loss at step 806000: 0.679142\n",
      "Average loss at step 808000: 0.679146\n",
      "Average loss at step 810000: 0.682989\n",
      "Average loss at step 812000: 0.680320\n",
      "Average loss at step 814000: 0.677761\n",
      "Average loss at step 816000: 0.676305\n",
      "Average loss at step 818000: 0.673775\n",
      "Average loss at step 820000: 0.677175\n",
      "Average loss at step 822000: 0.677021\n",
      "Average loss at step 824000: 0.672915\n",
      "Average loss at step 826000: 0.675222\n",
      "Average loss at step 828000: 0.674115\n",
      "Average loss at step 830000: 0.672943\n",
      "Average loss at step 832000: 0.677270\n",
      "Average loss at step 834000: 0.676966\n",
      "Average loss at step 836000: 0.673192\n",
      "Average loss at step 838000: 0.673858\n",
      "Average loss at step 840000: 0.673449\n",
      "Average loss at step 842000: 0.669506\n",
      "Average loss at step 844000: 0.673093\n",
      "Average loss at step 846000: 0.670617\n",
      "Average loss at step 848000: 0.670287\n",
      "Average loss at step 850000: 0.669553\n",
      "Average loss at step 852000: 0.670501\n",
      "Average loss at step 854000: 0.670103\n",
      "Average loss at step 856000: 0.668183\n",
      "Average loss at step 858000: 0.673281\n",
      "Average loss at step 860000: 0.669892\n",
      "Average loss at step 862000: 0.665482\n",
      "Average loss at step 864000: 0.667955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 866000: 0.668447\n",
      "Average loss at step 868000: 0.663036\n",
      "Average loss at step 870000: 0.668445\n",
      "Average loss at step 872000: 0.670659\n",
      "Average loss at step 874000: 0.665958\n",
      "Average loss at step 876000: 0.667863\n",
      "Average loss at step 878000: 0.665420\n",
      "Average loss at step 880000: 0.665032\n",
      "Average loss at step 882000: 0.666761\n",
      "Average loss at step 884000: 0.666745\n",
      "Average loss at step 886000: 0.665975\n",
      "Average loss at step 888000: 0.661084\n",
      "Average loss at step 890000: 0.667517\n",
      "Average loss at step 892000: 0.665986\n",
      "Average loss at step 894000: 0.661554\n",
      "Average loss at step 896000: 0.665704\n",
      "Average loss at step 898000: 0.666041\n",
      "Average loss at step 900000: 0.664562\n",
      "Average loss at step 902000: 0.660119\n",
      "Average loss at step 904000: 0.661879\n",
      "Average loss at step 906000: 0.661480\n",
      "Average loss at step 908000: 0.659583\n",
      "Average loss at step 910000: 0.660673\n",
      "Average loss at step 912000: 0.656590\n",
      "Average loss at step 914000: 0.658720\n",
      "Average loss at step 916000: 0.663280\n",
      "Average loss at step 918000: 0.658025\n",
      "Average loss at step 920000: 0.657092\n",
      "Average loss at step 922000: 0.660674\n",
      "Average loss at step 924000: 0.658057\n",
      "Average loss at step 926000: 0.661956\n",
      "Average loss at step 928000: 0.660325\n",
      "Average loss at step 930000: 0.656220\n",
      "Average loss at step 932000: 0.660780\n",
      "Average loss at step 934000: 0.655533\n",
      "Average loss at step 936000: 0.655328\n",
      "Average loss at step 938000: 0.657015\n",
      "Average loss at step 940000: 0.658061\n",
      "Average loss at step 942000: 0.656840\n",
      "Average loss at step 944000: 0.654657\n",
      "Average loss at step 946000: 0.654570\n",
      "Average loss at step 948000: 0.655948\n",
      "Average loss at step 950000: 0.656817\n",
      "Average loss at step 952000: 0.654925\n",
      "Average loss at step 954000: 0.656041\n",
      "Average loss at step 956000: 0.650694\n",
      "Average loss at step 958000: 0.655508\n",
      "Average loss at step 960000: 0.654133\n",
      "Average loss at step 962000: 0.653113\n",
      "Average loss at step 964000: 0.651440\n",
      "Average loss at step 966000: 0.652933\n",
      "Average loss at step 968000: 0.650815\n",
      "Average loss at step 970000: 0.653530\n",
      "Average loss at step 972000: 0.651364\n",
      "Average loss at step 974000: 0.650944\n",
      "Average loss at step 976000: 0.651645\n",
      "Average loss at step 978000: 0.651144\n",
      "Average loss at step 980000: 0.647347\n",
      "Average loss at step 982000: 0.651360\n",
      "Average loss at step 984000: 0.649206\n",
      "Average loss at step 986000: 0.650245\n",
      "Average loss at step 988000: 0.649613\n",
      "Average loss at step 990000: 0.646514\n",
      "Average loss at step 992000: 0.652108\n",
      "Average loss at step 994000: 0.649532\n",
      "Average loss at step 996000: 0.648818\n",
      "Average loss at step 998000: 0.648627\n",
      "Average loss at step 1000000: 0.645301\n",
      "Average loss at step 1002000: 0.647425\n",
      "Average loss at step 1004000: 0.645764\n",
      "Average loss at step 1006000: 0.647843\n",
      "Average loss at step 1008000: 0.648936\n",
      "Average loss at step 1010000: 0.649621\n",
      "Average loss at step 1012000: 0.644928\n",
      "Average loss at step 1014000: 0.644630\n",
      "Average loss at step 1016000: 0.646550\n",
      "Average loss at step 1018000: 0.645666\n",
      "Average loss at step 1020000: 0.643459\n",
      "Average loss at step 1022000: 0.644174\n",
      "Average loss at step 1024000: 0.645676\n",
      "Average loss at step 1026000: 0.645033\n",
      "Average loss at step 1028000: 0.640842\n",
      "Average loss at step 1030000: 0.643644\n",
      "Average loss at step 1032000: 0.643499\n",
      "Average loss at step 1034000: 0.644726\n",
      "Average loss at step 1036000: 0.641623\n",
      "Average loss at step 1038000: 0.643375\n",
      "Average loss at step 1040000: 0.642467\n",
      "Average loss at step 1042000: 0.645949\n",
      "Average loss at step 1044000: 0.643337\n",
      "Average loss at step 1046000: 0.642708\n",
      "Average loss at step 1048000: 0.639936\n",
      "Average loss at step 1050000: 0.639504\n",
      "Average loss at step 1052000: 0.639530\n",
      "Average loss at step 1054000: 0.643197\n",
      "Average loss at step 1056000: 0.641005\n",
      "Average loss at step 1058000: 0.638337\n",
      "Average loss at step 1060000: 0.638282\n",
      "Average loss at step 1062000: 0.640434\n",
      "Average loss at step 1064000: 0.636230\n",
      "Average loss at step 1066000: 0.638599\n",
      "Average loss at step 1068000: 0.634863\n",
      "Average loss at step 1070000: 0.635846\n",
      "Average loss at step 1072000: 0.636343\n",
      "Average loss at step 1074000: 0.637029\n",
      "Average loss at step 1076000: 0.637569\n",
      "Average loss at step 1078000: 0.635500\n",
      "Average loss at step 1080000: 0.636497\n",
      "Average loss at step 1082000: 0.637965\n",
      "Average loss at step 1084000: 0.634384\n",
      "Average loss at step 1086000: 0.633269\n",
      "Average loss at step 1088000: 0.634302\n",
      "Average loss at step 1090000: 0.637554\n",
      "Average loss at step 1092000: 0.636183\n",
      "Average loss at step 1094000: 0.634160\n",
      "Average loss at step 1096000: 0.635438\n",
      "Average loss at step 1098000: 0.632304\n",
      "Average loss at step 1100000: 0.635520\n",
      "Average loss at step 1102000: 0.632688\n",
      "Average loss at step 1104000: 0.634209\n",
      "Average loss at step 1106000: 0.634938\n",
      "Average loss at step 1108000: 0.635825\n",
      "Average loss at step 1110000: 0.631448\n",
      "Average loss at step 1112000: 0.634159\n",
      "Average loss at step 1114000: 0.630628\n",
      "Average loss at step 1116000: 0.629548\n",
      "Average loss at step 1118000: 0.634316\n",
      "Average loss at step 1120000: 0.631974\n",
      "Average loss at step 1122000: 0.631254\n",
      "Average loss at step 1124000: 0.629930\n",
      "Average loss at step 1126000: 0.628107\n",
      "Average loss at step 1128000: 0.629684\n",
      "Average loss at step 1130000: 0.630099\n",
      "Average loss at step 1132000: 0.629543\n",
      "Average loss at step 1134000: 0.631982\n",
      "Average loss at step 1136000: 0.628730\n",
      "Average loss at step 1138000: 0.627092\n",
      "Average loss at step 1140000: 0.625972\n",
      "Average loss at step 1142000: 0.629486\n",
      "Average loss at step 1144000: 0.628318\n",
      "Average loss at step 1146000: 0.630454\n",
      "Average loss at step 1148000: 0.628066\n",
      "Average loss at step 1150000: 0.626220\n",
      "Average loss at step 1152000: 0.629036\n",
      "Average loss at step 1154000: 0.624267\n",
      "Average loss at step 1156000: 0.621426\n",
      "Average loss at step 1158000: 0.626284\n",
      "Average loss at step 1160000: 0.625850\n",
      "Average loss at step 1162000: 0.626979\n",
      "Average loss at step 1164000: 0.630289\n",
      "Average loss at step 1166000: 0.623677\n",
      "Average loss at step 1168000: 0.628003\n",
      "Average loss at step 1170000: 0.624706\n",
      "Average loss at step 1172000: 0.623956\n",
      "Average loss at step 1174000: 0.626404\n",
      "Average loss at step 1176000: 0.626600\n",
      "Average loss at step 1178000: 0.623382\n",
      "Average loss at step 1180000: 0.619952\n",
      "Average loss at step 1182000: 0.622823\n",
      "Average loss at step 1184000: 0.619069\n",
      "Average loss at step 1186000: 0.622209\n",
      "Average loss at step 1188000: 0.623213\n",
      "Average loss at step 1190000: 0.623350\n",
      "Average loss at step 1192000: 0.624543\n",
      "Average loss at step 1194000: 0.619329\n",
      "Average loss at step 1196000: 0.620743\n",
      "Average loss at step 1198000: 0.621459\n",
      "Average loss at step 1200000: 0.621894\n",
      "Average loss at step 1202000: 0.616482\n",
      "Average loss at step 1204000: 0.619761\n",
      "Average loss at step 1206000: 0.625090\n",
      "Average loss at step 1208000: 0.620798\n",
      "Average loss at step 1210000: 0.620280\n",
      "Average loss at step 1212000: 0.621679\n",
      "Average loss at step 1214000: 0.617674\n",
      "Average loss at step 1216000: 0.618156\n",
      "Average loss at step 1218000: 0.619464\n",
      "Average loss at step 1220000: 0.617351\n",
      "Average loss at step 1222000: 0.617838\n",
      "Average loss at step 1224000: 0.620331\n",
      "Average loss at step 1226000: 0.618266\n",
      "Average loss at step 1228000: 0.616675\n",
      "Average loss at step 1230000: 0.619285\n",
      "Average loss at step 1232000: 0.613905\n",
      "Average loss at step 1234000: 0.614711\n",
      "Average loss at step 1236000: 0.617174\n",
      "Average loss at step 1238000: 0.617048\n",
      "Average loss at step 1240000: 0.616029\n",
      "Average loss at step 1242000: 0.615896\n",
      "Average loss at step 1244000: 0.613717\n",
      "Average loss at step 1246000: 0.614878\n",
      "Average loss at step 1248000: 0.616223\n",
      "Average loss at step 1250000: 0.615674\n",
      "Average loss at step 1252000: 0.617136\n",
      "Average loss at step 1254000: 0.613428\n",
      "Average loss at step 1256000: 0.615744\n",
      "Average loss at step 1258000: 0.614765\n",
      "Average loss at step 1260000: 0.613553\n",
      "Average loss at step 1262000: 0.613160\n",
      "Average loss at step 1264000: 0.612154\n",
      "Average loss at step 1266000: 0.611418\n",
      "Average loss at step 1268000: 0.613159\n",
      "Average loss at step 1270000: 0.616032\n",
      "Average loss at step 1272000: 0.615233\n",
      "Average loss at step 1274000: 0.613927\n",
      "Average loss at step 1276000: 0.611145\n",
      "Average loss at step 1278000: 0.611845\n",
      "Average loss at step 1280000: 0.611456\n",
      "Average loss at step 1282000: 0.610411\n",
      "Average loss at step 1284000: 0.612292\n",
      "Average loss at step 1286000: 0.609966\n",
      "Average loss at step 1288000: 0.611208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1290000: 0.611017\n",
      "Average loss at step 1292000: 0.612055\n",
      "Average loss at step 1294000: 0.608301\n",
      "Average loss at step 1296000: 0.610407\n",
      "Average loss at step 1298000: 0.609990\n",
      "Average loss at step 1300000: 0.607151\n",
      "Average loss at step 1302000: 0.607364\n",
      "Average loss at step 1304000: 0.610947\n",
      "Average loss at step 1306000: 0.607453\n",
      "Average loss at step 1308000: 0.609217\n",
      "Average loss at step 1310000: 0.608229\n",
      "Average loss at step 1312000: 0.608581\n",
      "Average loss at step 1314000: 0.607843\n",
      "Average loss at step 1316000: 0.607318\n",
      "Average loss at step 1318000: 0.605893\n",
      "Average loss at step 1320000: 0.607051\n",
      "Average loss at step 1322000: 0.608400\n",
      "Average loss at step 1324000: 0.606001\n",
      "Average loss at step 1326000: 0.606604\n",
      "Average loss at step 1328000: 0.609204\n",
      "Average loss at step 1330000: 0.606466\n",
      "Average loss at step 1332000: 0.606786\n",
      "Average loss at step 1334000: 0.607493\n",
      "Average loss at step 1336000: 0.603157\n",
      "Average loss at step 1338000: 0.603626\n",
      "Average loss at step 1340000: 0.604622\n",
      "Average loss at step 1342000: 0.602446\n",
      "Average loss at step 1344000: 0.602481\n",
      "Average loss at step 1346000: 0.601015\n",
      "Average loss at step 1348000: 0.603473\n",
      "Average loss at step 1350000: 0.607661\n",
      "Average loss at step 1352000: 0.602520\n",
      "Average loss at step 1354000: 0.602548\n",
      "Average loss at step 1356000: 0.606205\n",
      "Average loss at step 1358000: 0.602916\n",
      "Average loss at step 1360000: 0.602728\n",
      "Average loss at step 1362000: 0.600543\n",
      "Average loss at step 1364000: 0.600157\n",
      "Average loss at step 1366000: 0.598740\n",
      "Average loss at step 1368000: 0.602117\n",
      "Average loss at step 1370000: 0.602520\n",
      "Average loss at step 1372000: 0.601205\n",
      "Average loss at step 1374000: 0.602257\n",
      "Average loss at step 1376000: 0.599317\n",
      "Average loss at step 1378000: 0.597750\n",
      "Average loss at step 1380000: 0.601087\n",
      "Average loss at step 1382000: 0.600208\n",
      "Average loss at step 1384000: 0.598838\n",
      "Average loss at step 1386000: 0.597409\n",
      "Average loss at step 1388000: 0.597511\n",
      "Average loss at step 1390000: 0.598642\n",
      "Average loss at step 1392000: 0.599535\n",
      "Average loss at step 1394000: 0.598166\n",
      "Average loss at step 1396000: 0.598386\n",
      "Average loss at step 1398000: 0.598909\n",
      "Average loss at step 1400000: 0.599324\n",
      "Average loss at step 1402000: 0.597590\n",
      "Average loss at step 1404000: 0.599305\n",
      "Average loss at step 1406000: 0.599849\n",
      "Average loss at step 1408000: 0.594608\n",
      "Average loss at step 1410000: 0.597518\n",
      "Average loss at step 1412000: 0.597461\n",
      "Average loss at step 1414000: 0.596148\n",
      "Average loss at step 1416000: 0.592715\n",
      "Average loss at step 1418000: 0.597645\n",
      "Average loss at step 1420000: 0.597587\n",
      "Average loss at step 1422000: 0.593061\n",
      "Average loss at step 1424000: 0.593426\n",
      "Average loss at step 1426000: 0.596665\n",
      "Average loss at step 1428000: 0.596383\n",
      "Average loss at step 1430000: 0.592732\n",
      "Average loss at step 1432000: 0.596413\n",
      "Average loss at step 1434000: 0.596190\n",
      "Average loss at step 1436000: 0.593238\n",
      "Average loss at step 1438000: 0.590753\n",
      "Average loss at step 1440000: 0.594866\n",
      "Average loss at step 1442000: 0.593223\n",
      "Average loss at step 1444000: 0.593976\n",
      "Average loss at step 1446000: 0.595289\n",
      "Average loss at step 1448000: 0.590376\n",
      "Average loss at step 1450000: 0.595749\n",
      "Average loss at step 1452000: 0.594057\n",
      "Average loss at step 1454000: 0.592898\n",
      "Average loss at step 1456000: 0.590336\n",
      "Average loss at step 1458000: 0.592046\n",
      "Average loss at step 1460000: 0.593507\n",
      "Average loss at step 1462000: 0.591365\n",
      "Average loss at step 1464000: 0.588434\n",
      "Average loss at step 1466000: 0.591885\n",
      "Average loss at step 1468000: 0.591748\n",
      "Average loss at step 1470000: 0.587549\n",
      "Average loss at step 1472000: 0.589004\n",
      "Average loss at step 1474000: 0.590908\n",
      "Average loss at step 1476000: 0.589830\n",
      "Average loss at step 1478000: 0.586957\n",
      "Average loss at step 1480000: 0.586811\n",
      "Average loss at step 1482000: 0.591522\n",
      "Average loss at step 1484000: 0.589105\n",
      "Average loss at step 1486000: 0.588970\n",
      "Average loss at step 1488000: 0.588936\n",
      "Average loss at step 1490000: 0.589705\n",
      "Average loss at step 1492000: 0.588589\n",
      "Average loss at step 1494000: 0.589695\n",
      "Average loss at step 1496000: 0.588622\n",
      "Average loss at step 1498000: 0.590258\n",
      "Average loss at step 1500000: 0.589572\n",
      "Average loss at step 1502000: 0.588573\n",
      "Average loss at step 1504000: 0.588314\n",
      "Average loss at step 1506000: 0.586697\n",
      "Average loss at step 1508000: 0.589037\n",
      "Average loss at step 1510000: 0.582646\n",
      "Average loss at step 1512000: 0.584847\n",
      "Average loss at step 1514000: 0.585507\n",
      "Average loss at step 1516000: 0.585920\n",
      "Average loss at step 1518000: 0.582239\n",
      "Average loss at step 1520000: 0.587891\n",
      "Average loss at step 1522000: 0.584353\n",
      "Average loss at step 1524000: 0.583500\n",
      "Average loss at step 1526000: 0.585405\n",
      "Average loss at step 1528000: 0.582477\n",
      "Average loss at step 1530000: 0.584262\n",
      "Average loss at step 1532000: 0.587661\n",
      "Average loss at step 1534000: 0.585347\n",
      "Average loss at step 1536000: 0.584156\n",
      "Average loss at step 1538000: 0.587461\n",
      "Average loss at step 1540000: 0.584190\n",
      "Average loss at step 1542000: 0.588131\n",
      "Average loss at step 1544000: 0.584005\n",
      "Average loss at step 1546000: 0.584903\n",
      "Average loss at step 1548000: 0.583228\n",
      "Average loss at step 1550000: 0.582943\n",
      "Average loss at step 1552000: 0.582543\n",
      "Average loss at step 1554000: 0.584225\n",
      "Average loss at step 1556000: 0.585151\n",
      "Average loss at step 1558000: 0.582689\n",
      "Average loss at step 1560000: 0.582818\n",
      "Average loss at step 1562000: 0.584378\n",
      "Average loss at step 1564000: 0.584528\n",
      "Average loss at step 1566000: 0.582957\n",
      "Average loss at step 1568000: 0.582019\n",
      "Average loss at step 1570000: 0.581631\n",
      "Average loss at step 1572000: 0.582927\n",
      "Average loss at step 1574000: 0.580994\n",
      "Average loss at step 1576000: 0.578867\n",
      "Average loss at step 1578000: 0.578382\n",
      "Average loss at step 1580000: 0.581804\n",
      "Average loss at step 1582000: 0.580607\n",
      "Average loss at step 1584000: 0.577586\n",
      "Average loss at step 1586000: 0.578856\n",
      "Average loss at step 1588000: 0.581179\n",
      "Average loss at step 1590000: 0.581258\n",
      "Average loss at step 1592000: 0.576709\n",
      "Average loss at step 1594000: 0.578568\n",
      "Average loss at step 1596000: 0.578425\n",
      "Average loss at step 1598000: 0.580683\n",
      "Average loss at step 1600000: 0.579591\n",
      "Average loss at step 1602000: 0.578233\n",
      "Average loss at step 1604000: 0.577523\n",
      "Average loss at step 1606000: 0.580204\n",
      "Average loss at step 1608000: 0.580973\n",
      "Average loss at step 1610000: 0.579432\n",
      "Average loss at step 1612000: 0.575664\n",
      "Average loss at step 1614000: 0.572783\n",
      "Average loss at step 1616000: 0.577928\n",
      "Average loss at step 1618000: 0.576946\n",
      "Average loss at step 1620000: 0.577702\n",
      "Average loss at step 1622000: 0.576434\n",
      "Average loss at step 1624000: 0.575559\n",
      "Average loss at step 1626000: 0.577959\n",
      "Average loss at step 1628000: 0.576386\n",
      "Average loss at step 1630000: 0.576317\n",
      "Average loss at step 1632000: 0.570832\n",
      "Average loss at step 1634000: 0.576983\n",
      "Average loss at step 1636000: 0.572329\n",
      "Average loss at step 1638000: 0.577340\n",
      "Average loss at step 1640000: 0.574358\n",
      "Average loss at step 1642000: 0.578072\n",
      "Average loss at step 1644000: 0.575462\n",
      "Average loss at step 1646000: 0.573454\n",
      "Average loss at step 1648000: 0.572595\n",
      "Average loss at step 1650000: 0.571607\n",
      "Average loss at step 1652000: 0.571206\n",
      "Average loss at step 1654000: 0.570141\n",
      "Average loss at step 1656000: 0.571447\n",
      "Average loss at step 1658000: 0.572485\n",
      "Average loss at step 1660000: 0.575449\n",
      "Average loss at step 1662000: 0.571722\n",
      "Average loss at step 1664000: 0.570709\n",
      "Average loss at step 1666000: 0.571357\n",
      "Average loss at step 1668000: 0.575603\n",
      "Average loss at step 1670000: 0.573747\n",
      "Average loss at step 1672000: 0.570726\n",
      "Average loss at step 1674000: 0.570924\n",
      "Average loss at step 1676000: 0.569763\n",
      "Average loss at step 1678000: 0.568264\n",
      "Average loss at step 1680000: 0.568032\n",
      "Average loss at step 1682000: 0.572758\n",
      "Average loss at step 1684000: 0.572909\n",
      "Average loss at step 1686000: 0.569635\n",
      "Average loss at step 1688000: 0.571716\n",
      "Average loss at step 1690000: 0.570422\n",
      "Average loss at step 1692000: 0.568543\n",
      "Average loss at step 1694000: 0.568154\n",
      "Average loss at step 1696000: 0.569917\n",
      "Average loss at step 1698000: 0.570557\n",
      "Average loss at step 1700000: 0.566949\n",
      "Average loss at step 1702000: 0.570159\n",
      "Average loss at step 1704000: 0.570386\n",
      "Average loss at step 1706000: 0.569798\n",
      "Average loss at step 1708000: 0.565347\n",
      "Average loss at step 1710000: 0.565780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1712000: 0.568911\n",
      "Average loss at step 1714000: 0.567186\n",
      "Average loss at step 1716000: 0.566784\n",
      "Average loss at step 1718000: 0.566183\n",
      "Average loss at step 1720000: 0.566542\n",
      "Average loss at step 1722000: 0.566288\n",
      "Average loss at step 1724000: 0.566696\n",
      "Average loss at step 1726000: 0.567790\n",
      "Average loss at step 1728000: 0.568059\n",
      "Average loss at step 1730000: 0.564736\n",
      "Average loss at step 1732000: 0.567455\n",
      "Average loss at step 1734000: 0.565555\n",
      "Average loss at step 1736000: 0.569313\n",
      "Average loss at step 1738000: 0.567603\n",
      "Average loss at step 1740000: 0.569599\n",
      "Average loss at step 1742000: 0.570211\n",
      "Average loss at step 1744000: 0.566529\n",
      "Average loss at step 1746000: 0.562760\n",
      "Average loss at step 1748000: 0.563628\n",
      "Average loss at step 1750000: 0.567290\n",
      "Average loss at step 1752000: 0.567181\n",
      "Average loss at step 1754000: 0.566608\n",
      "Average loss at step 1756000: 0.566368\n",
      "Average loss at step 1758000: 0.563448\n",
      "Average loss at step 1760000: 0.562838\n",
      "Average loss at step 1762000: 0.563155\n",
      "Average loss at step 1764000: 0.563911\n",
      "Average loss at step 1766000: 0.566382\n",
      "Average loss at step 1768000: 0.563871\n",
      "Average loss at step 1770000: 0.566130\n",
      "Average loss at step 1772000: 0.561645\n",
      "Average loss at step 1774000: 0.561418\n",
      "Average loss at step 1776000: 0.565040\n",
      "Average loss at step 1778000: 0.563296\n",
      "Average loss at step 1780000: 0.560668\n",
      "Average loss at step 1782000: 0.561668\n",
      "Average loss at step 1784000: 0.563451\n",
      "Average loss at step 1786000: 0.562847\n",
      "Average loss at step 1788000: 0.559786\n",
      "Average loss at step 1790000: 0.563348\n",
      "Average loss at step 1792000: 0.559293\n",
      "Average loss at step 1794000: 0.563033\n",
      "Average loss at step 1796000: 0.563017\n",
      "Average loss at step 1798000: 0.560956\n",
      "Average loss at step 1800000: 0.559951\n",
      "Average loss at step 1802000: 0.561364\n",
      "Average loss at step 1804000: 0.563617\n",
      "Average loss at step 1806000: 0.559525\n",
      "Average loss at step 1808000: 0.561923\n",
      "Average loss at step 1810000: 0.561653\n",
      "Average loss at step 1812000: 0.562314\n",
      "Average loss at step 1814000: 0.560941\n",
      "Average loss at step 1816000: 0.561124\n",
      "Average loss at step 1818000: 0.560864\n",
      "Average loss at step 1820000: 0.558381\n",
      "Average loss at step 1822000: 0.560613\n",
      "Average loss at step 1824000: 0.560722\n",
      "Average loss at step 1826000: 0.557965\n",
      "Average loss at step 1828000: 0.562117\n",
      "Average loss at step 1830000: 0.556297\n",
      "Average loss at step 1832000: 0.557862\n",
      "Average loss at step 1834000: 0.557274\n",
      "Average loss at step 1836000: 0.556900\n",
      "Average loss at step 1838000: 0.556914\n",
      "Average loss at step 1840000: 0.559929\n",
      "Average loss at step 1842000: 0.556996\n",
      "Average loss at step 1844000: 0.560831\n",
      "Average loss at step 1846000: 0.553690\n",
      "Average loss at step 1848000: 0.556669\n",
      "Average loss at step 1850000: 0.558086\n",
      "Average loss at step 1852000: 0.556176\n",
      "Average loss at step 1854000: 0.557277\n",
      "Average loss at step 1856000: 0.555875\n",
      "Average loss at step 1858000: 0.555469\n",
      "Average loss at step 1860000: 0.556172\n",
      "Average loss at step 1862000: 0.558813\n",
      "Average loss at step 1864000: 0.560014\n",
      "Average loss at step 1866000: 0.560816\n",
      "Average loss at step 1868000: 0.553892\n",
      "Average loss at step 1870000: 0.554344\n",
      "Average loss at step 1872000: 0.556101\n",
      "Average loss at step 1874000: 0.555454\n",
      "Average loss at step 1876000: 0.554620\n",
      "Average loss at step 1878000: 0.557295\n",
      "Average loss at step 1880000: 0.556253\n",
      "Average loss at step 1882000: 0.556536\n",
      "Average loss at step 1884000: 0.556590\n",
      "Average loss at step 1886000: 0.552494\n",
      "Average loss at step 1888000: 0.555110\n",
      "Average loss at step 1890000: 0.552856\n",
      "Average loss at step 1892000: 0.550772\n",
      "Average loss at step 1894000: 0.556390\n",
      "Average loss at step 1896000: 0.554350\n",
      "Average loss at step 1898000: 0.551588\n",
      "Average loss at step 1900000: 0.552947\n",
      "Average loss at step 1902000: 0.552557\n",
      "Average loss at step 1904000: 0.553087\n",
      "Average loss at step 1906000: 0.552938\n",
      "Average loss at step 1908000: 0.551701\n",
      "Average loss at step 1910000: 0.548870\n",
      "Average loss at step 1912000: 0.553786\n",
      "Average loss at step 1914000: 0.549988\n",
      "Average loss at step 1916000: 0.553946\n",
      "Average loss at step 1918000: 0.548737\n",
      "Average loss at step 1920000: 0.550046\n",
      "Average loss at step 1922000: 0.548390\n",
      "Average loss at step 1924000: 0.551341\n",
      "Average loss at step 1926000: 0.553558\n",
      "Average loss at step 1928000: 0.551334\n",
      "Average loss at step 1930000: 0.549611\n",
      "Average loss at step 1932000: 0.548440\n",
      "Average loss at step 1934000: 0.551211\n",
      "Average loss at step 1936000: 0.548005\n",
      "Average loss at step 1938000: 0.550942\n",
      "Average loss at step 1940000: 0.551625\n",
      "Average loss at step 1942000: 0.551212\n",
      "Average loss at step 1944000: 0.546005\n",
      "Average loss at step 1946000: 0.552160\n",
      "Average loss at step 1948000: 0.552227\n",
      "Average loss at step 1950000: 0.548124\n",
      "Average loss at step 1952000: 0.548291\n",
      "Average loss at step 1954000: 0.549808\n",
      "Average loss at step 1956000: 0.548002\n",
      "Average loss at step 1958000: 0.546515\n",
      "Average loss at step 1960000: 0.550859\n",
      "Average loss at step 1962000: 0.547075\n",
      "Average loss at step 1964000: 0.550481\n",
      "Average loss at step 1966000: 0.549600\n",
      "Average loss at step 1968000: 0.549267\n",
      "Average loss at step 1970000: 0.547221\n",
      "Average loss at step 1972000: 0.545382\n",
      "Average loss at step 1974000: 0.546784\n",
      "Average loss at step 1976000: 0.548622\n",
      "Average loss at step 1978000: 0.546804\n",
      "Average loss at step 1980000: 0.547611\n",
      "Average loss at step 1982000: 0.547192\n",
      "Average loss at step 1984000: 0.548620\n",
      "Average loss at step 1986000: 0.543523\n",
      "Average loss at step 1988000: 0.547235\n",
      "Average loss at step 1990000: 0.547424\n",
      "Average loss at step 1992000: 0.546122\n",
      "Average loss at step 1994000: 0.546134\n",
      "Average loss at step 1996000: 0.545286\n",
      "Average loss at step 1998000: 0.544544\n",
      "Average loss at step 2000000: 0.546523\n",
      "Validation accuracy: 80.3%\n",
      "Model saved in path: ./models/model\n",
      "seconds  63766.17529654503\n",
      "minutes  1062.0\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2000001 # 3001 #\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), val_labels_one_hot))\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_train_batch(batch_size)\n",
    "        feed_dictionary = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} \n",
    "        tempVal = session.run([optimizer, loss], feed_dict=feed_dictionary)\n",
    "        l = tempVal[1]\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step %d: %f' % (step, average_loss))\n",
    "                average_loss = 0\n",
    "            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            \n",
    "    print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), val_labels_one_hot))\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(session, \"./models/model\")\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "end = time.time()\n",
    "seconds = end - start\n",
    "print(\"seconds \", seconds)\n",
    "print(\"minutes \", seconds//60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model\n",
      "Restoring data\n",
      "Validation accuracy: 80.3%\n",
      "Initialized\n",
      "Average loss at step 2000: 0.531395\n",
      "Average loss at step 4000: 0.520662\n",
      "Average loss at step 6000: 0.516523\n",
      "Average loss at step 8000: 0.511995\n",
      "Average loss at step 10000: 0.508726\n",
      "Average loss at step 12000: 0.506995\n",
      "Average loss at step 14000: 0.504791\n",
      "Average loss at step 16000: 0.504013\n",
      "Average loss at step 18000: 0.500729\n",
      "Average loss at step 20000: 0.499691\n",
      "Average loss at step 22000: 0.497117\n",
      "Average loss at step 24000: 0.496394\n",
      "Average loss at step 26000: 0.496032\n",
      "Average loss at step 28000: 0.495294\n",
      "Average loss at step 30000: 0.493351\n",
      "Average loss at step 32000: 0.492674\n",
      "Average loss at step 34000: 0.491275\n",
      "Average loss at step 36000: 0.490298\n",
      "Average loss at step 38000: 0.489996\n",
      "Average loss at step 40000: 0.488864\n",
      "Average loss at step 42000: 0.488384\n",
      "Average loss at step 44000: 0.488605\n",
      "Average loss at step 46000: 0.486936\n",
      "Average loss at step 48000: 0.487063\n",
      "Average loss at step 50000: 0.485772\n",
      "Average loss at step 52000: 0.485796\n",
      "Average loss at step 54000: 0.484096\n",
      "Average loss at step 56000: 0.483813\n",
      "Average loss at step 58000: 0.484049\n",
      "Average loss at step 60000: 0.484331\n",
      "Average loss at step 62000: 0.482305\n",
      "Average loss at step 64000: 0.482046\n",
      "Average loss at step 66000: 0.481659\n",
      "Average loss at step 68000: 0.481387\n",
      "Average loss at step 70000: 0.480403\n",
      "Average loss at step 72000: 0.480807\n",
      "Average loss at step 74000: 0.479886\n",
      "Average loss at step 76000: 0.479253\n",
      "Average loss at step 78000: 0.479386\n",
      "Average loss at step 80000: 0.479199\n",
      "Average loss at step 82000: 0.477386\n",
      "Average loss at step 84000: 0.477608\n",
      "Average loss at step 86000: 0.478518\n",
      "Average loss at step 88000: 0.476000\n",
      "Average loss at step 90000: 0.476083\n",
      "Average loss at step 92000: 0.476771\n",
      "Average loss at step 94000: 0.477397\n",
      "Average loss at step 96000: 0.475910\n",
      "Average loss at step 98000: 0.475916\n",
      "Average loss at step 100000: 0.475050\n",
      "Validation accuracy: 82.9%\n",
      "Model saved in path: ./models2/model\n",
      "seconds  19234.642347335815\n",
      "minutes  320.0\n"
     ]
    }
   ],
   "source": [
    "batch_size2 = 1024\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "session2 = tf.Session()\n",
    "\n",
    "new_saver = tf.train.import_meta_graph('./models/model.meta')\n",
    "new_saver.restore(session2, tf.train.latest_checkpoint('./models/'))\n",
    "\n",
    "print('Restoring data')\n",
    "\n",
    "graph2 = tf.get_default_graph()\n",
    "\n",
    "tf_train_dataset2 = tf.placeholder(tf.float32, shape=(batch_size2, num_input_dimensions))\n",
    "tf_train_labels2 = tf.placeholder(tf.float32, shape=(batch_size2, num_moves))\n",
    "tf_valid_dataset2 = tf.constant(val_data)\n",
    "\n",
    "W1 = graph2.get_tensor_by_name(\"W1:0\")\n",
    "b1 = graph2.get_tensor_by_name(\"b1:0\")\n",
    "\n",
    "W2 = graph2.get_tensor_by_name(\"W2:0\")\n",
    "b2 = graph2.get_tensor_by_name(\"b2:0\")\n",
    "\n",
    "W3 = graph2.get_tensor_by_name(\"W3:0\")\n",
    "b3 = graph2.get_tensor_by_name(\"b3:0\")\n",
    "\n",
    "W4 = graph2.get_tensor_by_name(\"W4:0\")\n",
    "b4 = graph2.get_tensor_by_name(\"b4:0\")\n",
    "\n",
    "def model2(data):\n",
    "    layer1_raw = tf.matmul(data, W1) + b1\n",
    "    layer1 = tf.nn.relu(layer1_raw)\n",
    "\n",
    "    layer2_raw = tf.matmul(layer1, W2) + b2\n",
    "    layer2 = tf.nn.relu(layer2_raw)\n",
    "\n",
    "    layer3_raw = tf.matmul(layer2, W3) + b3\n",
    "    layer3 = tf.nn.relu(layer3_raw)\n",
    "\n",
    "    logits = tf.matmul(layer3, W4) + b4\n",
    "\n",
    "    return logits\n",
    "    \n",
    "def compute_loss2(labels, logits):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "\n",
    "# Training computation.\n",
    "logits2 = model2(tf_train_dataset2)\n",
    "\n",
    "loss2 = compute_loss2(tf_train_labels2, logits2)\n",
    "\n",
    "optimizer2 = tf.train.GradientDescentOptimizer(0.01).minimize(loss2)\n",
    "\n",
    "valid_prediction2 = tf.nn.softmax(model2(tf_valid_dataset2)) \n",
    "\n",
    "print('Validation accuracy: %.1f%%' % accuracy(valid_prediction2.eval(session=session2), val_labels_one_hot))\n",
    "    \n",
    "num_steps = 100001\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "average_loss = 0\n",
    "for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_train_batch(batch_size2)\n",
    "    feed_dictionary = {tf_train_dataset2 : batch_data, tf_train_labels2 : batch_labels} \n",
    "    tempVal = session2.run([optimizer2, loss2], feed_dict=feed_dictionary)\n",
    "    l = tempVal[1]\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss = average_loss / 2000\n",
    "        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "\n",
    "print('Validation accuracy: %.1f%%' % accuracy(valid_prediction2.eval(session=session2), val_labels_one_hot))\n",
    "#saver3 = tf.train.Saver()\n",
    "save_path = new_saver.save(session2, \"./models2/model\")\n",
    "print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "end = time.time()\n",
    "seconds = end - start\n",
    "print(\"seconds \", seconds)\n",
    "print(\"minutes \", seconds//60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalancedData = transformWithoutBalansing(data2)\n",
    "random.shuffle(balancedData) #unbalancedData\n",
    "moveLabelsUnique, cubesAsVectorsUnique, states = transformDataIntoExamples(balancedData)#unbalancedData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(balancedData)#unbalancedData\n",
    "val_len = int(data_len * 0.01)\n",
    "\n",
    "val_data = cubesAsVectorsUnique[:val_len]\n",
    "val_states = states[:val_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models2/model\n",
      "Restoring data\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "session3 = tf.Session()\n",
    "\n",
    "new_saver = tf.train.import_meta_graph('./models2/model.meta')\n",
    "new_saver.restore(session3, tf.train.latest_checkpoint('./models2/'))\n",
    "\n",
    "print('Restoring data')\n",
    "\n",
    "graph3 = tf.get_default_graph()\n",
    "\n",
    "W1 = graph3.get_tensor_by_name(\"W1:0\")\n",
    "b1 = graph3.get_tensor_by_name(\"b1:0\")\n",
    "\n",
    "W2 = graph3.get_tensor_by_name(\"W2:0\")\n",
    "b2 = graph3.get_tensor_by_name(\"b2:0\")\n",
    "\n",
    "W3 = graph3.get_tensor_by_name(\"W3:0\")\n",
    "b3 = graph3.get_tensor_by_name(\"b3:0\")\n",
    "\n",
    "W4 = graph3.get_tensor_by_name(\"W4:0\")\n",
    "b4 = graph3.get_tensor_by_name(\"b4:0\")\n",
    "\n",
    "def model3(data):\n",
    "    layer1_raw = tf.matmul(data, W1) + b1\n",
    "    layer1 = tf.nn.relu(layer1_raw)\n",
    "\n",
    "    layer2_raw = tf.matmul(layer1, W2) + b2\n",
    "    layer2 = tf.nn.relu(layer2_raw)\n",
    "\n",
    "    layer3_raw = tf.matmul(layer2, W3) + b3\n",
    "    layer3 = tf.nn.relu(layer3_raw)\n",
    "\n",
    "    logits = tf.matmul(layer3, W4) + b4\n",
    "\n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98185\n",
      "Step done:  0\n",
      "Step done:  1\n",
      "Step done:  2\n",
      "Step done:  3\n",
      "Step done:  4\n",
      "Step done:  5\n",
      "Step done:  6\n",
      "Step done:  7\n",
      "Step done:  8\n",
      "Step done:  9\n",
      "Step done:  10\n",
      "Step done:  11\n",
      "Step done:  12\n",
      "Step done:  13\n",
      "Step done:  14\n",
      "Step done:  15\n",
      "Step done:  16\n",
      "Step done:  17\n",
      "Step done:  18\n",
      "Step done:  19\n",
      "Step done:  20\n",
      "Step done:  21\n",
      "Step done:  22\n",
      "Step done:  23\n",
      "Step done:  24\n",
      "Step done:  25\n",
      "Step done:  26\n",
      "Step done:  27\n",
      "Step done:  28\n",
      "Step done:  29\n",
      "Step done:  30\n",
      "Step done:  31\n",
      "Step done:  32\n",
      "Step done:  33\n",
      "Step done:  34\n",
      "Step done:  35\n",
      "Step done:  36\n",
      "Step done:  37\n",
      "Step done:  38\n",
      "Step done:  39\n",
      "seconds  1753.1131119728088\n",
      "minutes  29.0\n"
     ]
    }
   ],
   "source": [
    "current_test_data = val_data\n",
    "current_test_data_as_strings = val_states\n",
    "\n",
    "print(val_len)\n",
    "max_moves = 40\n",
    "\n",
    "results = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for step in range(max_moves):\n",
    "    results.append(current_test_data)\n",
    "    tf_test_iteration = tf.constant(current_test_data)\n",
    "    #print(current_test_data[0:2])\n",
    "    iteration_logits = tf.nn.softmax(model3(tf_test_iteration)).eval(session=session3)\n",
    "    iteration_moves = np.argmax(iteration_logits, axis=1)\n",
    "    \n",
    "    tempArr = list(map(performCubeAntiMovement, current_test_data_as_strings, iteration_moves))\n",
    "    mapped_states_vector = np.asarray(list([ f for (f, _) in tempArr]))\n",
    "    mapped_states_string = [ s for (_, s) in tempArr]\n",
    "    #new_states = np.asarray(list(mappedStates))\n",
    "    \n",
    "    current_test_data = mapped_states_vector\n",
    "    current_test_data_as_strings = mapped_states_string\n",
    "    \n",
    "    print(\"Step done: \", step)\n",
    "\n",
    "end = time.time()\n",
    "seconds = end - start\n",
    "print(\"seconds \", seconds)\n",
    "print(\"minutes \", seconds//60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_len 98185\n",
      "solved %:  88.61027651881652\n"
     ]
    }
   ],
   "source": [
    "test_length_final = val_len\n",
    "solved_states = 0\n",
    "print(\"val_len\", test_length_final)\n",
    "solved_state = convertStringToVector(Rubik2().getStateCompact())\n",
    "#print(solved_state)\n",
    "almost_solved = convertStringToVector(Rubik2().performMove(5).getStateCompact())\n",
    "results[2][2] = almost_solved\n",
    "\n",
    "for test in range(test_length_final):\n",
    "    #if test % 100 == 0: print(\"test \", test)\n",
    "    \n",
    "    for move in range(max_moves):\n",
    "        state = results[move][test]\n",
    "        if(np.array_equal(state,solved_state)):\n",
    "            #print(\"Solved: Test, move:\", test, move)\n",
    "            solved_states += 1\n",
    "            break\n",
    "            \n",
    "print(\"solved %: \", solved_states / test_length_final * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwewrtryuioqwertyuiqwgertyuiqwertyuiiuytrew\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2), (3, 1), (2, 2)]\n",
      "['rrwwyyrrooyywwoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg', 'oowwwwrrrryyyyoobbbbgggg', 'yywwoorrwwyyrroobbbbgggg']\n"
     ]
    }
   ],
   "source": [
    "mappedPath = [ data2[convertVectorToString(step[11])]  for step in results]\n",
    "print(mappedPath)\n",
    "mappedPath = [ convertVectorToString(step[11])  for step in results]\n",
    "print(mappedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "[11, 12, 1, 12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6]\n",
      "[10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9]\n",
      "[10, 11, 10, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9]\n",
      "[9, 10, 11, 10, 9, 10, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[10, 11, 12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[13, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[10, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[11, 12, 11, 12, 11, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9]\n",
      "[5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[12, 11, 10, 11, 12, 11, 12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9]\n",
      "[12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[11, 10, 9, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]\n",
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6]\n",
      "[12, 11, 10, 9, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7]\n",
      "[10, 11, 10, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "[5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4]\n",
      "[6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7]\n",
      "[9, 10, 11, 10, 9, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[13, 12, 11, 12, 11, 10, 11, 12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[13, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6]\n",
      "[11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[10, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6]\n",
      "[6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7]\n",
      "[5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6]\n",
      "[5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6]\n",
      "[9, 10, 9, 10, 11, 10, 11, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9]\n",
      "[10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]\n",
      "[12, 11, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9]\n",
      "[11, 12, 13, 12, 11, 12, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[13, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12]\n",
      "[11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[13, 12, 11, 12, 11, 12, 13, 12, 11, 10, 9, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n",
      "[11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10]\n",
      "[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "[11, 10, 11, 10, 11, 12, 11, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9]\n",
      "[9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "[10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]\n",
      "[7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]\n",
      "[6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5]\n",
      "[3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2]\n",
      "[9, 10, 11, 10, 11, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10]\n",
      "[10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11, 10, 11]\n",
      "[11, 10, 9, 10, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12]\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
      "[5, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6]\n",
      "[11, 12, 11, 12, 11, 10, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12]\n",
      "[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    mappedPath = [ data2[convertVectorToString(step[i])][1]  for step in results]\n",
    "    print(mappedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
